{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "df = pd.read_csv(\"train.csv\", header = 0)\n",
    "display(df.std())\n",
    "\n",
    "total_images = len(df)\n",
    "\n",
    "# drop rows with new_whale because it is used to label various unknown flukes yet\n",
    "df = df[df.Id != 'new_whale']\n",
    "\n",
    "# use targets with 3+ samples\n",
    "df = df.groupby(\"Id\").filter(lambda x: len(x) >= 3)\n",
    "\n",
    "# Use top N targets by image count\n",
    "value_counts = df.Id.value_counts()\n",
    "top_hitters = value_counts.nlargest(20).index\n",
    "df = df[df['Id'].isin(top_hitters)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel(\"Whales\")\n",
    "ax.set_xlabel(\"Images per whale\")\n",
    "value_counts.hist(ax=ax,figsize=(20,5),bins=10, bottom=1)\n",
    "\n",
    "classes = df.Id.unique()\n",
    "num_classes = len(classes)\n",
    "\n",
    "X_train = []; y_train = []\n",
    "\n",
    "X_test = []; y_test = []\n",
    "\n",
    "X_valid = []; y_valid = []\n",
    "\n",
    "for whale_id in classes:\n",
    "    df_whale = df[df.Id == whale_id]\n",
    "    \n",
    "    X_whale = np.array([os.path.join(os.getcwd(), 'train', s) for s in df_whale.Image])\n",
    "    y_whale = np.array(df_whale.Id.values)\n",
    "    \n",
    "    X_train_whale, X_test_whale, y_train_whale, y_test_whale = \\\n",
    "        train_test_split(X_whale, y_whale, test_size=0.2, random_state=1)\n",
    "    X_test.extend(X_test_whale)\n",
    "    y_test.extend(y_test_whale)\n",
    "    \n",
    "    X_train_whale, X_valid_whale, y_train_whale, y_valid_whale = \\\n",
    "        train_test_split(X_train_whale, y_train_whale, test_size=0.2, random_state=1)\n",
    "    X_train.extend(X_train_whale)\n",
    "    y_train.extend(y_train_whale)\n",
    "    X_valid.extend(X_valid_whale)\n",
    "    y_valid.extend(y_valid_whale)\n",
    "\n",
    "print('\\nThere are %d total images.' % total_images)\n",
    "print(\"Trainable...\")\n",
    "print('There are %d total classes.' % num_classes)\n",
    "\n",
    "print('There are %d training images.' % len(X_train))\n",
    "print('There are %d validation images.' % len(X_valid))\n",
    "print('There are %d test images.' % len(X_test))\n",
    "\n",
    "print(\"After data filtering\")\n",
    "\n",
    "df.describe()\n",
    "\n",
    "workers = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the 12 random training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "def visualize_img(img_path, ax):\n",
    "    img = cv2.imread(img_path)\n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "fig = plt.figure(figsize=(50, 30))\n",
    "\n",
    "num_images = 12\n",
    "rand_images = random.sample(X_train, num_images)\n",
    "for i in range(num_images):\n",
    "    ax = fig.add_subplot(3, 4, i + 1, xticks=[], yticks=[])\n",
    "    visualize_img(rand_images[i], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset for ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "def copy_class_of_files(files, dst, labels):\n",
    "    for idx, val in enumerate(files):\n",
    "        dst_dir = os.path.join(dst, labels[idx])\n",
    "        pathlib.Path(dst_dir).mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy(val, dst_dir)\n",
    "        \n",
    "shutil.rmtree('./data', ignore_errors=True)\n",
    "copy_class_of_files(X_train, 'data/train', y_train)\n",
    "copy_class_of_files(X_valid, 'data/valid', y_valid)\n",
    "copy_class_of_files(X_test, 'data/test', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CNN without top layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "# define InceptionResNetV2 model\n",
    "image_side_size = 299\n",
    "image_dim = (image_side_size, image_side_size, 3)\n",
    "base_model = InceptionResNetV2(weights='imagenet', include_top=False,\n",
    "                          input_tensor=None, input_shape = image_dim, \n",
    "                          pooling=None)\n",
    "\n",
    "# freeze weights in all layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "                      \n",
    "# Train several last layers in base model\n",
    "# for layer in base_model.layers[-3:]:\n",
    "#     layer.trainable = True\n",
    "    \n",
    "#base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training and validation image generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 1\n",
    "target_size = (image_side_size, image_side_size)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        horizontal_flip=False,\n",
    "        vertical_flip=True,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "print(\"Train generator\")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'data/train',\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "print(\"Valid generator\")\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "        'data/valid',\n",
    "        target_size = target_size,\n",
    "        class_mode = None,\n",
    "        batch_size = batch_size,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record bottleneck features of the base pre-trained model\n",
    "https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_train = base_model.predict_generator(\n",
    "    train_generator, \n",
    "    workers=workers, verbose=1, \n",
    "    steps=train_generator.samples//batch_size)\n",
    "\n",
    "# save the output as a Numpy array\n",
    "np.save('bottleneck_features_train.npy', bottleneck_features_train)\n",
    "\n",
    "bottleneck_features_validation = base_model.predict_generator(\n",
    "    valid_generator,\n",
    "    workers=workers, verbose=1, \n",
    "    steps=valid_generator.samples//batch_size)\n",
    "\n",
    "np.save('bottleneck_features_validation.npy', bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train top level dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import utils as np_utils\n",
    "\n",
    "train_data = np.load('bottleneck_features_train.npy')\n",
    "validation_data = np.load('bottleneck_features_validation.npy')\n",
    "\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "top_model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "#model.summary()\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "best_weights_path='whale.flukes.weights.best.hdf5'\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=best_weights_path, verbose=1, save_best_only=True)\n",
    "\n",
    "# Stop the training if the model shows no improvement \n",
    "stopper = EarlyStopping(monitor='val_loss', min_delta=0.005, patience=25, verbose=1, mode='auto')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "top_model.fit(x=train_data,  \n",
    "          y=np_utils.to_categorical(label_encoder.fit_transform(y_train)),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[checkpointer, stopper],\n",
    "          validation_data=(validation_data, \n",
    "                           np_utils.to_categorical(\n",
    "                               label_encoder.fit_transform(y_valid))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "top_model.load_weights(best_weights_path)\n",
    "\n",
    "model = Model(input=base_model.input, \n",
    "              output=top_model(base_model.output))\n",
    "\n",
    "layers_to_freeze = len(model.layers) - len(top_model.layers)\n",
    "\n",
    "print(f\"Freezing {layers_to_freeze} layers\")\n",
    "\n",
    "for layer in model.layers[:layers_to_freeze]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[layers_to_freeze:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "#model.summary()\n",
    "\n",
    "print(\"Traning dataset...\")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'data/train',\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        class_mode=\"categorical\",\n",
    "        shuffle=False)\n",
    "\n",
    "print(\"Validation dataset...\")\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "        'data/valid',\n",
    "        target_size = target_size,\n",
    "        class_mode = \"categorical\",\n",
    "        batch_size = batch_size,\n",
    "        shuffle=False)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "tuned_checkpointer = ModelCheckpoint(filepath='whale.flukes.weights.tuned.hdf5', \n",
    "                                     verbose=1, \n",
    "                                     save_best_only=True)\n",
    "\n",
    "stopper = EarlyStopping(monitor='val_loss', \n",
    "                        min_delta=0.005, \n",
    "                        patience=20, \n",
    "                        verbose=1, \n",
    "                        mode='auto')\n",
    "\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch = int(len(X_train)//batch_size),\n",
    "                    validation_data=valid_generator, \n",
    "                    validation_steps = int(len(X_valid)//batch_size),\n",
    "                    callbacks=[tuned_checkpointer, stopper],\n",
    "                    verbose=1, workers=workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Classification Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('whale.flukes.weights.tuned.hdf5')\n",
    "#model.summary()\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    \"data/test\",\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical', shuffle=False)\n",
    "\n",
    "score = model.evaluate_generator(\n",
    "    generator=test_generator, \n",
    "    #steps=len(test_generator.classes)//min(len(test_generator.classes),32),\n",
    "    workers=workers)\n",
    "\n",
    "print(\"Loss: \", score[0], \"Accuracy: \", score[1])\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    \"data/test\",\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical', shuffle=False)\n",
    "\n",
    "scores = model.predict_generator(\n",
    "    generator=test_generator, \n",
    "    workers=workers,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "print(len(list(map(lambda x: np.argmax(x), scores))))\n",
    "\n",
    "cnf_matrix = confusion_matrix(test_generator.classes, list(map(lambda x: np.argmax(x), scores)))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(70,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes, title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(70,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes, normalize=True, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write classification result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import ntpath\n",
    "\n",
    "with open('predictions.csv', 'w') as csvfile:\n",
    "    fieldnames = ['Image', 'Id']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for idx, file_name in enumerate(X_test):\n",
    "        probs = scores[idx]\n",
    "        _, sorted_classes = zip(*sorted(zip(probs, classes), reverse=True))\n",
    "        writer.writerow(dict(zip(fieldnames, [ntpath.basename(file_name), ' '.join(sorted_classes[:3])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whale-flukes",
   "language": "python",
   "name": "whale-flukes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
