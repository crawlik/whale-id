{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLossesKeras\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import cv2\n",
    "import random\n",
    "import shutil\n",
    "import pathlib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\", header = 0)\n",
    "display(df.std())\n",
    "\n",
    "total_images = len(df)\n",
    "\n",
    "# drop rows with new_whale because it is used to label various unknown flukes yet\n",
    "df = df[df.Id != 'new_whale']\n",
    "\n",
    "# use targets with 3+ samples\n",
    "df = df.groupby(\"Id\").filter(lambda x: len(x) >= 3)\n",
    "\n",
    "# Use top N targets by image count\n",
    "value_counts = df.Id.value_counts()\n",
    "top_hitters = value_counts.nlargest(60).index\n",
    "df = df[df['Id'].isin(top_hitters)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel(\"Whales\")\n",
    "ax.set_xlabel(\"Images per whale\")\n",
    "value_counts.hist(ax=ax,figsize=(20,5),bins=10, bottom=1)\n",
    "\n",
    "classes = df.Id.unique()\n",
    "num_classes = len(classes)\n",
    "\n",
    "X_train = []; y_train = []\n",
    "\n",
    "X_test = []; y_test = []\n",
    "\n",
    "X_valid = []; y_valid = []\n",
    "\n",
    "for whale_id in classes:\n",
    "    df_whale = df[df.Id == whale_id]\n",
    "    \n",
    "    X_whale = np.array([os.path.join(os.getcwd(), 'train', s) for s in df_whale.Image])\n",
    "    y_whale = np.array(df_whale.Id.values)\n",
    "    \n",
    "    X_train_whale, X_test_whale, y_train_whale, y_test_whale = \\\n",
    "        train_test_split(X_whale, y_whale, test_size=0.2, random_state=1)\n",
    "    X_test.extend(X_test_whale)\n",
    "    y_test.extend(y_test_whale)\n",
    "    \n",
    "    X_train_whale, X_valid_whale, y_train_whale, y_valid_whale = \\\n",
    "        train_test_split(X_train_whale, y_train_whale, test_size=0.2, random_state=1)\n",
    "    X_train.extend(X_train_whale)\n",
    "    y_train.extend(y_train_whale)\n",
    "    X_valid.extend(X_valid_whale)\n",
    "    y_valid.extend(y_valid_whale)\n",
    "\n",
    "print('\\nThere are %d total images.' % total_images)\n",
    "print(\"Trainable...\")\n",
    "print('There are %d total classes.' % num_classes)\n",
    "\n",
    "print('There are %d training images.' % len(X_train))\n",
    "print('There are %d validation images.' % len(X_valid))\n",
    "print('There are %d test images.' % len(X_test))\n",
    "\n",
    "print(\"After data filtering\")\n",
    "\n",
    "df.describe()\n",
    "\n",
    "workers = 8\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the First 12 Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_img(img_path, ax):\n",
    "    img = cv2.imread(img_path)\n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "fig = plt.figure(figsize=(50, 30))\n",
    "\n",
    "num_images = 12\n",
    "rand_images = random.sample(X_train, num_images)\n",
    "for i in range(num_images):\n",
    "    ax = fig.add_subplot(3, 4, i + 1, xticks=[], yticks=[])\n",
    "    visualize_img(rand_images[i], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset for ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_class_of_files(files, dst, labels):\n",
    "    for idx, val in enumerate(files):\n",
    "        dst_dir = os.path.join(dst, labels[idx])\n",
    "        pathlib.Path(dst_dir).mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy(val, dst_dir)\n",
    "        \n",
    "shutil.rmtree('./data', ignore_errors=True)\n",
    "copy_class_of_files(X_train, 'data/train', y_train)\n",
    "copy_class_of_files(X_valid, 'data/valid', y_valid)\n",
    "copy_class_of_files(X_test, 'data/test', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CNN without top layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "\n",
    "#define VGG19 model\n",
    "image_side_size = 150\n",
    "image_dim = (image_side_size, image_side_size, 3)\n",
    "base_model = VGG19(weights='imagenet', include_top=False,\n",
    "                          input_tensor=None, input_shape=image_dim, \n",
    "                          pooling=None)\n",
    "cut_off_layer = base_model.get_layer('block4_pool')\n",
    "\n",
    "# define InceptionResNetV2 model\n",
    "# image_side_size = 299\n",
    "# image_dim = (image_side_size, image_side_size, 3)\n",
    "# base_model = InceptionResNetV2(weights='imagenet', include_top=False,\n",
    "#                           input_tensor=None, input_shape=image_dim, \n",
    "#                           pooling=None)\n",
    "# cut_off_layer = base_model.get_layer('mixed_7a')\n",
    "\n",
    "print(f\"Layers {len(base_model.layers)}\")\n",
    "\n",
    "# freeze weights in all layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(\"Complete model before trimming\")\n",
    "base_model.summary()\n",
    "\n",
    "\n",
    "# https://github.com/keras-team/keras/issues/8909#issuecomment-354406145\n",
    "# https://github.com/keras-team/keras/issues/2371\n",
    "base_model = Model(base_model.input, cut_off_layer.output)\n",
    "\n",
    "print(f'Last base layer: {base_model.output}')\n",
    "\n",
    "print(\"Trimmed model\")\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top layer specific to our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=RMSprop(lr=0.0001), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define image generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "target_size = (image_side_size, image_side_size)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "print(\"Train generator\")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'data/train',\n",
    "        target_size = target_size,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "print(\"Valid generator\")\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "        'data/valid',\n",
    "        target_size = target_size,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "\n",
    "best_weights_path='whale.flukes.weights.best.hdf5'\n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath=best_weights_path, verbose=1, save_best_only=True)\n",
    "\n",
    "# Stop the training if the model shows no improvement \n",
    "stopper = EarlyStopping(monitor='val_loss', min_delta=0.005, patience=100, verbose=1, mode='auto')\n",
    "\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    epochs=1000,\n",
    "                    steps_per_epoch = len(X_train)//batch_size,\n",
    "                    validation_data=valid_generator, \n",
    "                    validation_steps = len(X_valid)//batch_size,\n",
    "                    callbacks=[checkpointer, stopper, \n",
    "                               PlotLossesKeras(), \n",
    "                               TensorBoard(log_dir='./logs', batch_size=batch_size)], \n",
    "                    verbose=1, workers=workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(best_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Classification Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    \"data/test\",\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical', shuffle=False)\n",
    "\n",
    "score = model.evaluate_generator(\n",
    "    generator=test_generator, \n",
    "    workers=workers)\n",
    "\n",
    "print(\"Loss: \", score[0], \"Accuracy: \", score[1])\n",
    "\n",
    "scores = model.predict_generator(\n",
    "    generator=test_generator, \n",
    "    workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "cnf_matrix = confusion_matrix(test_generator.classes, list(map(lambda x: np.argmax(x), scores)))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(80,20))\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes, title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(80,20))\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes, normalize=True, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write classification result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import ntpath\n",
    "\n",
    "with open('predictions.csv', 'w') as csvfile:\n",
    "    fieldnames = ['Image', 'Id']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for idx, file_name in enumerate(X_test):\n",
    "        probs = scores[idx]\n",
    "        _, sorted_classes = zip(*sorted(zip(probs, classes), reverse=True))\n",
    "        writer.writerow(dict(zip(fieldnames, [ntpath.basename(file_name), ' '.join(sorted_classes[:3])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whale-id",
   "language": "python",
   "name": "whale-id"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
