{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLossesKeras\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import cv2\n",
    "import random\n",
    "import shutil\n",
    "import pathlib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "batch_size = 64\n",
    "image_side_size = 150\n",
    "\n",
    "target_size = (image_side_size, image_side_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\", header = 0)\n",
    "\n",
    "total_images = len(df)\n",
    "\n",
    "# drop rows with new_whale because it is used to label various unknown flukes yet\n",
    "df = df[df.Id != 'new_whale']\n",
    "\n",
    "image_threshhold = 3\n",
    "\n",
    "# under represented targets\n",
    "df_under_targets = df.groupby(\"Id\").filter(lambda x: len(x) < image_threshhold)\n",
    "\n",
    "# use targets with image_threshhold+ samples\n",
    "df = df.groupby(\"Id\").filter(lambda x: len(x) >= image_threshhold)\n",
    "\n",
    "value_counts = df.Id.value_counts()\n",
    "\n",
    "top_hitters = value_counts.nlargest(10).index\n",
    "\n",
    "# Use top N targets by image count\n",
    "#df = df[df['Id'].isin(top_hitters)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel(\"Whales\")\n",
    "ax.set_xlabel(\"Images per whale\")\n",
    "value_counts.hist(ax=ax, figsize=(20,5), bins=100, bottom=1)\n",
    "\n",
    "classes = df.Id.unique()\n",
    "num_classes = len(classes)\n",
    "\n",
    "X_train = []; y_train = []\n",
    "\n",
    "X_test = []; y_test = []\n",
    "\n",
    "X_valid = []; y_valid = []\n",
    "\n",
    "for whale_id in classes:\n",
    "    df_whale = df[df.Id == whale_id]\n",
    "    \n",
    "    X_whale = np.array([os.path.join(os.getcwd(), 'train', s) for s in df_whale.Image])\n",
    "    y_whale = np.array(df_whale.Id.values)\n",
    "    \n",
    "    X_train_whale, X_test_whale, y_train_whale, y_test_whale = \\\n",
    "        train_test_split(X_whale, y_whale, test_size=0.2, random_state=1)\n",
    "    X_test.extend(X_test_whale)\n",
    "    y_test.extend(y_test_whale)\n",
    "    \n",
    "    X_train_whale, X_valid_whale, y_train_whale, y_valid_whale = \\\n",
    "        train_test_split(X_train_whale, y_train_whale, test_size=0.2, random_state=1)\n",
    "    X_train.extend(X_train_whale)\n",
    "    y_train.extend(y_train_whale)\n",
    "    X_valid.extend(X_valid_whale)\n",
    "    y_valid.extend(y_valid_whale)\n",
    "\n",
    "print('\\nThere are %d total images.' % total_images)\n",
    "print(\"\\nTrainable...\")\n",
    "print('There are %d total classes.' % num_classes)\n",
    "\n",
    "print('There are %d training actual images.' % len(X_train))\n",
    "print('There are %d validation actual images.' % len(X_valid))\n",
    "print('There are %d test actual images.' % len(X_test))\n",
    "\n",
    "print(\"After data filtering\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nUnder represented targets\")\n",
    "print(df_under_targets.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate under represented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=False,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "augmented_path = './augmented'\n",
    "\n",
    "if not os.path.exists(augmented_path):\n",
    "\n",
    "    pathlib.Path(augmented_path).mkdir(parents=True)\n",
    "\n",
    "    for index, row in df_under_targets.iterrows():\n",
    "        file = row['Image']\n",
    "        id = row['Id']\n",
    "\n",
    "        img = load_img(f'train/{file}')\n",
    "        img = img_to_array(img) \n",
    "        img = img.reshape((1,) + img.shape)\n",
    "\n",
    "        # the .flow() command below generates batches of randomly transformed images\n",
    "        i = 0\n",
    "        for batch in datagen.flow(img,\n",
    "                                  batch_size=1,\n",
    "                                  save_to_dir=augmented_path, \n",
    "                                  save_prefix=f'{id}', \n",
    "                                  save_format='jpeg'):\n",
    "            i += 1\n",
    "            if i > 5:\n",
    "                break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build labeled list of augmented images for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "import ntpath\n",
    "\n",
    "augmented_file = 'train_augmented.csv'\n",
    "\n",
    "if not os.path.isfile(augmented_file): \n",
    "    with open('train_augmented.csv', 'w') as csvfile:\n",
    "        fieldnames = ['Image', 'Id']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        augmented_list = glob.glob(f\"{augmented_path}/*.jpeg\")\n",
    "\n",
    "        for file in augmented_list:\n",
    "            base_name = os.path.basename(file)\n",
    "            writer.writerow(dict(zip(fieldnames,[base_name, \"_\".join(base_name.split(\"_\", 2)[:2])])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribute augmented files across labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented = pd.read_csv(\"train_augmented.csv\", header = 0)\n",
    "\n",
    "total_augmented_images = len(df_augmented)\n",
    "\n",
    "classes_augmented = df_augmented.Id.unique()\n",
    "num_augmented_classes = len(classes_augmented)\n",
    "\n",
    "for whale_id in classes_augmented:\n",
    "    df_whale = df_augmented[df_augmented.Id == whale_id]\n",
    "    \n",
    "    X_whale = np.array([os.path.join(os.getcwd(), 'augmented', s) for s in df_whale.Image])\n",
    "    y_whale = np.array(df_whale.Id.values)\n",
    "    \n",
    "    X_train_whale, X_test_whale, y_train_whale, y_test_whale = \\\n",
    "        train_test_split(X_whale, y_whale, test_size=0.2, random_state=1)\n",
    "    X_test.extend(X_test_whale)\n",
    "    y_test.extend(y_test_whale)\n",
    "    \n",
    "    X_train_whale, X_valid_whale, y_train_whale, y_valid_whale = \\\n",
    "        train_test_split(X_train_whale, y_train_whale, test_size=0.2, random_state=1)\n",
    "    X_train.extend(X_train_whale)\n",
    "    y_train.extend(y_train_whale)\n",
    "    X_valid.extend(X_valid_whale)\n",
    "    y_valid.extend(y_valid_whale)\n",
    "\n",
    "print('\\nThere are %d total augmented images.' % total_augmented_images)\n",
    "print('There are %d total augmented classes.' % num_augmented_classes)\n",
    "\n",
    "print('There are %d training images.' % len(X_train))\n",
    "print('There are %d validation images.' % len(X_valid))\n",
    "print('There are %d test images.' % len(X_test))\n",
    "\n",
    "num_classes += num_augmented_classes\n",
    "print('There are %d total classes.' % num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize 12 random training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_img(img_path, ax):\n",
    "    img = cv2.imread(img_path)\n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "fig = plt.figure(figsize=(50, 30))\n",
    "\n",
    "num_images = 12\n",
    "rand_images = random.sample(X_train, num_images)\n",
    "\n",
    "for i, file in enumerate(rand_images):\n",
    "    ax = fig.add_subplot(3, 4, i + 1, xticks=[], yticks=[])\n",
    "    visualize_img(file, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize random whale from top hitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_whale = random.sample(list(top_hitters),1)\n",
    "print(random_whale)\n",
    "\n",
    "whale_images = list(df[df['Id'].isin(random_whale)]['Image'])\n",
    "files = [s for s in X_train if any(w in s for w in whale_images)]\n",
    "\n",
    "fig = plt.figure(figsize=(50, 30))\n",
    "\n",
    "for i, file in enumerate(files[:12]):\n",
    "    ax = fig.add_subplot(3, 4, i + 1, xticks=[], yticks=[])\n",
    "    visualize_img(file, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset for ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_class_of_files(files, dst, labels):\n",
    "    for idx, val in enumerate(files):\n",
    "        dst_dir = os.path.join(dst, labels[idx])\n",
    "        pathlib.Path(dst_dir).mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy(val, dst_dir)\n",
    "        \n",
    "shutil.rmtree('./data', ignore_errors=True)\n",
    "copy_class_of_files(X_train, 'data/train', y_train)\n",
    "copy_class_of_files(X_valid, 'data/valid', y_valid)\n",
    "copy_class_of_files(X_test, 'data/test', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define image generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "print(\"Train generator\")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'data/train',\n",
    "        target_size = target_size,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "print(\"Valid generator\")\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "        'data/valid',\n",
    "        target_size = target_size,\n",
    "        class_mode = 'categorical',\n",
    "        batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CNN without top layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "\n",
    "# define base model\n",
    "image_dim = (image_side_size, image_side_size, 3)\n",
    "base_model = VGG16(weights='imagenet', include_top=False,\n",
    "                          input_tensor=None, input_shape=image_dim, \n",
    "                          pooling=None)\n",
    "cut_off_layer = base_model.get_layer('block4_conv3')\n",
    "\n",
    "# define InceptionResNetV2 model\n",
    "# image_side_size = 299\n",
    "# image_dim = (image_side_size, image_side_size, 3)\n",
    "# base_model = InceptionResNetV2(weights='imagenet', include_top=False,\n",
    "#                           input_tensor=None, input_shape=image_dim, \n",
    "#                           pooling=None)\n",
    "# cut_off_layer = base_model.get_layer('mixed_7a')\n",
    "\n",
    "print(f\"Layers {len(base_model.layers)}\")\n",
    "\n",
    "# freeze weights in all layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(\"Complete model before trimming\")\n",
    "base_model.summary()\n",
    "\n",
    "# https://github.com/keras-team/keras/issues/8909#issuecomment-354406145\n",
    "# https://github.com/keras-team/keras/issues/2371\n",
    "base_model = Model(base_model.input, cut_off_layer.output)\n",
    "\n",
    "print(f'Last base layer: {base_model.output}')\n",
    "\n",
    "print(\"Trimmed model\")\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top layer specific to the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "num_classes = len(np.unique(train_generator.classes))\n",
    "train_examples = len(train_generator.classes)\n",
    "\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=RMSprop(lr=0.0001), \n",
    "              metrics=['accuracy','mean_squared_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from time import time\n",
    "\n",
    "workers = 8\n",
    "\n",
    "best_weights_path=f'whale.flukes.{num_classes}_classes.weights.best.hdf5'\n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath=best_weights_path, verbose=1, save_best_only=True)\n",
    "\n",
    "# Stop the training if the model shows no improvement \n",
    "stopper = EarlyStopping(monitor='val_loss', \n",
    "                        min_delta=0.005, \n",
    "                        patience=50, \n",
    "                        verbose=1, \n",
    "                        mode='auto')\n",
    "\n",
    "# Reduce learning rate when a metric has stopped improving.\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                             factor=0.7,\n",
    "                             patience=10,\n",
    "                             min_lr=0.0000001,\n",
    "                             verbose=1)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"./logs/final/{}\".format(time()),\n",
    "                          batch_size=batch_size,\n",
    "                          write_graph=True,\n",
    "                          write_images=True,\n",
    "                          histogram_freq=0,\n",
    "                          write_grads=True)\n",
    "\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    epochs=500,\n",
    "                    steps_per_epoch = train_examples//batch_size,\n",
    "                    validation_data=valid_generator, \n",
    "                    validation_steps = train_examples//batch_size,\n",
    "                    callbacks=[checkpointer, stopper, reduce_lr,\n",
    "                               PlotLossesKeras(), \n",
    "                               tensorboard],\n",
    "                    verbose=1, workers=workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('whale.flukes.4250_classes.weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Classification Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    \"data/test\",\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical', shuffle=False)\n",
    "\n",
    "score = model.evaluate_generator(\n",
    "    generator=test_generator)\n",
    "\n",
    "print(\"Loss: \", score[0], \"Accuracy: \", score[1])\n",
    "\n",
    "scores = model.predict_generator(\n",
    "    generator=test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genegate confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(test_generator.classes, list(map(lambda x: np.argmax(x), scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot_confusion_heatmap(cm):\n",
    "    plt.imshow(cm, interpolation='none')\n",
    "    plt.colorbar()\n",
    "    \n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "# plt.figure(figsize=(20,20))\n",
    "# plot_confusion_matrix(cnf_matrix, classes=classes, title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "# plt.figure(figsize=(20,20))\n",
    "# plot_confusion_matrix(cnf_matrix, classes=classes, normalize=True, title='Normalized confusion matrix')\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plot_confusion_heatmap(cnf_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write classification result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import ntpath\n",
    "\n",
    "with open('predictions.csv', 'w') as csvfile:\n",
    "    fieldnames = ['Image', 'Id']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for idx, file_name in enumerate(X_test):\n",
    "        probs = scores[idx]\n",
    "        _, sorted_classes = zip(*sorted(zip(probs, classes), reverse=True))\n",
    "        writer.writerow(dict(zip(fieldnames, [ntpath.basename(file_name), ' '.join(sorted_classes[:3])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whale-id",
   "language": "python",
   "name": "whale-id"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
