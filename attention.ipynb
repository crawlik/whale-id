{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on https://github.com/raghakot/keras-vis/blob/master/examples/vggnet/attention.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention on VGGNet (Saliency and grad-CAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize activation over final dense layer outputs, we need to switch the `softmax` activation out for `linear` since gradient of output node will depend on all the other node activations. Doing this in keras is tricky, so we provide `utils.apply_modifications` to modify network parameters and rebuild the graph.\n",
    "\n",
    "If this swapping is not done, the results might be suboptimal. We will start by swapping out 'softmax' for 'linear'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from vis.utils import utils\n",
    "from keras import activations\n",
    "import random\n",
    "\n",
    "model = load_model('whale.flukes.4250_classes.weights.best.hdf5')\n",
    "\n",
    "prediction_layer = 'dense_3'\n",
    "layer_idx = utils.find_layer_idx(model, prediction_layer)\n",
    "# Swap softmax with linear\n",
    "model.layers[layer_idx].activation = activations.linear\n",
    "model = utils.apply_modifications(model)\n",
    "\n",
    "all_classes = np.load('train_classes.npy')\n",
    "\n",
    "num_classes = len(all_classes)\n",
    "\n",
    "target_size = (150, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load a couple of test images to try saliency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from vis.utils import utils\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (18, 6)\n",
    "\n",
    "whale_id = random.sample(list(all_classes), 1)[0]\n",
    "whale_index = np.where(all_classes==whale_id)[0][0]\n",
    "\n",
    "print(f'Whale id {whale_id}')\n",
    "\n",
    "files = glob.glob(f'data/test/{whale_id}/*')\n",
    "if len(files) < 2:\n",
    "    files.append(files[0])\n",
    "    \n",
    "img1 = utils.load_img(files[0], target_size=target_size)\n",
    "img2 = utils.load_img(files[1], target_size=target_size)\n",
    "\n",
    "f, ax = plt.subplots(1, 2)\n",
    "\n",
    "ax[0].imshow(img1)\n",
    "ax[1].imshow(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for saliency visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis.visualization import visualize_saliency, overlay\n",
    "from vis.utils import utils\n",
    "from keras import activations\n",
    "\n",
    "f, ax = plt.subplots(1, 2)\n",
    "for i, img in enumerate([img1, img2]):    \n",
    "    grads = visualize_saliency(model, layer_idx, filter_indices=whale_index, seed_input=img)\n",
    "    \n",
    "    # visualize grads as heatmap\n",
    "    ax[i].imshow(grads, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not that great. Very noisy. Lets try guided and rectified saliency.\n",
    "\n",
    "To use guided saliency, we need to set `backprop_modifier='guided'`. For rectified saliency or deconv saliency, use `backprop_modifier='relu'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for modifier in ['guided', 'relu']:\n",
    "    plt.figure()\n",
    "    f, ax = plt.subplots(1, 2)\n",
    "    plt.suptitle(modifier)\n",
    "    for i, img in enumerate([img1, img2]):    \n",
    "        grads = visualize_saliency(model, layer_idx, filter_indices=whale_index, \n",
    "                                   seed_input=img, backprop_modifier=modifier)\n",
    "        # Lets overlay the heatmap onto original image.    \n",
    "        ax[i].imshow(grads, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "guided saliency is definitely better. I am not sure whats going on with rectified saliency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grad-CAM - vanilla, guided, rectified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These should contain more detail since they use `Conv` or `Pooling` features that contain more spatial detail which is lost in `Dense` layers. The only additional detail compared to saliency is the `penultimate_layer_idx`. This specifies the pre-layer whose gradients should be used. See this paper for technical details: https://arxiv.org/pdf/1610.02391v1.pdf\n",
    "\n",
    "By default, if `penultimate_layer_idx` is not defined, it searches for the nearest pre layer. For our architecture, that would be the `block5_pool` layer after all the `Conv` layers. Here is the model summary for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "from vis.visualization import visualize_cam\n",
    "\n",
    "for modifier in [None, 'guided', 'relu']:\n",
    "    plt.figure()\n",
    "    f, ax = plt.subplots(1, 2)\n",
    "    plt.suptitle(\"vanilla\" if modifier is None else modifier)\n",
    "    for i, img in enumerate([img1, img2]):    \n",
    "        grads = visualize_cam(model, layer_idx, filter_indices=whale_index, \n",
    "                              seed_input=img, backprop_modifier=modifier)        \n",
    "        # Lets overlay the heatmap onto original image.    \n",
    "        jet_heatmap = np.uint8(cm.jet(grads)[..., :3] * 255)\n",
    "        #ax[i].imshow(overlay(jet_heatmap, img)) https://github.com/raghakot/keras-vis/issues/73\n",
    "        ax[i].imshow(overlay(grads, img))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whale-id",
   "language": "python",
   "name": "whale-id"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
